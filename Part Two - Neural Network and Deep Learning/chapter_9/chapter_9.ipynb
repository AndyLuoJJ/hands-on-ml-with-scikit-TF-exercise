{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9: Running TensorFlow\n",
    "\n",
    "This chapter introduces how to run TensorFlow on your computer, and how to create a computing graph with TensorFlow. For further details on the APIs and higher level usages, please refer to official documentation or other TensorFlow tutorials.\n",
    "\n",
    "> This jupyter notebook contains my solution to coding exercises of this chapter. For answers to the questions, please check the markdown file under the same folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9: TensorFlow on Moon dataset\n",
    "\n",
    "Requirement: Use minibatch gradient descent to realize logistic regression. Train and evaluate on the Moon Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's do some preparation before completing the tasks below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "data, target = make_moons(n_samples=10000, noise=0.4)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target)\n",
    "y_train = np.reshape(y_train, (-1, 1))\n",
    "y_test = np.reshape(y_test, (-1, 1))\n",
    "\n",
    "# set up hyperparameters\n",
    "batch_size = 64\n",
    "num_epochs = 3000\n",
    "learning_rate = 0.001\n",
    "\n",
    "num_train_samples, num_features = X_train.shape[0], X_train.shape[1]\n",
    "num_batches = int(np.ceil(num_train_samples / batch_size))\n",
    "\n",
    "current_epoch = -1\n",
    "random_indexes = np.arange(num_train_samples)\n",
    "np.random.shuffle(random_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1: Define a reusable graph in method ```logistic_regression()```.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def logistic_regression(X):\n",
    "    with tf.name_scope(\"logistic_regression\") as scope:\n",
    "        W = tf.Variable(tf.random_uniform([2, 1], -1.0, 1.0), name=\"weights\")\n",
    "        b = tf.Variable(0.0, name=\"bias\")\n",
    "        output_tensor = tf.sigmoid(tf.add(tf.matmul(X, W), b))\n",
    "        \n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2: During the training progress, use ```Saver``` to save the checkpoint on time, and save the final model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "input_tensor = tf.placeholder(tf.float32, shape=(None, 2), name=\"input_tensor\")\n",
    "label = tf.placeholder(tf.float32, shape=(None, 1), name=\"label\")\n",
    "y = logistic_regression(input_tensor)\n",
    "\n",
    "with tf.name_scope('loss') as scope:\n",
    "    error = y - label\n",
    "    mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "    \n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "mse_summary = tf.summary.scaler('MSE', mse)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # check whether there is an existing model\n",
    "    if os.path.exists(\"/Users/Antinomy/Desktop/tmp/checkpoint\"):\n",
    "        saver.restore(sess, \"/Users/Antinomy/Desktop/tmp/my_model.ckpt\")\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        for j in range(num_batches):\n",
    "            if i != current_epoch:\n",
    "                random_indexes = np.arange(num_train_samples)\n",
    "                np.random.shuffle(random_indexes)\n",
    "                current_epoch += 1\n",
    "                \n",
    "            selected_indexes = random_indexes[(j*batch_size):((j+1)*batch_size)]\n",
    "            X_train_batch, y_train_batch = X_train[selected_indexes], y_train[selected_indexes]\n",
    "            if j % 10 == 0:\n",
    "                summary_str = mse_summary.eval(feed_dict={input_tensor: X_train_batch, label: y_train_batch})\n",
    "                step = i*num_batches + j\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            \n",
    "            sess.run(training_op, feed_dict={input_tensor: X_train_batch, label: y_train_batch})\n",
    "        \n",
    "        print(\"---------- Epoch %d ----------\" % i)\n",
    "        print(\"Loss:\", mse.eval(feed_dict={input_tensor: X_train_batch, label: y_train_batch}))\n",
    "        save_path = saver.save(sess, \"/Users/Antinomy/Desktop/tmp/my_model.ckpt\")\n",
    "        \n",
    "    saver.save(sess, \"/Users/Antinomy/Desktop/final/my_final_model.ckpt\")\n",
    "    file_writer.close()\n",
    "    \n",
    "    # make predictions with the trained graph\n",
    "    predictions = y.eval(feed_dict={input_tensor: X_test})\n",
    "    final_predictions = [int(item > 0.5) for item in predictions]\n",
    "    print(\"========== Result on Test Set ==========\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, final_predictions))\n",
    "    print(\"AUC:\", roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3: If the training progress has been terminated, restore the saved model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is shown below.\n",
    "``` python\n",
    "if os.path.exists(\"/Users/Antinomy/Desktop/tmp/checkpoint\"):\n",
    "        saver.restore(sess, \"/Users/Antinomy/Desktop/tmp/my_model.ckpt\")\n",
    "```\n",
    "The code snippet first checks whether the checkpoint file exists, then use ```tf.train.Saver``` to restore the previous session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4: Define the graph with appropriate domain to make it more decent in TensorBoard.**\n",
    "\n",
    "Let's first check how the graph looks like in TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"/Users/Antinomy/Desktop/tf_logs\"\n",
    "logdir = \"{}/run-{}\".format(root_logdir, now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the graph more decent, we can organize different operators under the same namespace. The code is shown below.\n",
    "``` python\n",
    "[...]\n",
    "def logistic_regression(X):\n",
    "    with tf.name_scope(\"logistic_regression\") as scope:\n",
    "        W = tf.Variable(tf.random_uniform([2, 1], -1.0, 1.0), name=\"weights\")\n",
    "        b = tf.Variable(0.0, name=\"bias\")\n",
    "        output_tensor = tf.sigmoid(tf.add(tf.matmul(X, W), b))\n",
    "        \n",
    "    return output_tensor\n",
    "\n",
    "[...]\n",
    "with tf.name_scope('loss') as scope:\n",
    "    error = y - label\n",
    "    mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "[...]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The complete code is shown below.* Hyperparameters are not perfectly tuned but the accuracy score and AUC score are both acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# prepare for dataset\n",
    "data, target = make_moons(n_samples=10000, noise=0.4)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target)\n",
    "y_train = np.reshape(y_train, (-1, 1))\n",
    "y_test = np.reshape(y_test, (-1, 1))\n",
    "\n",
    "# set up hyperparameters\n",
    "batch_size = 64\n",
    "num_epochs = 3000\n",
    "learning_rate = 0.001\n",
    "\n",
    "num_train_samples, num_features = X_train.shape[0], X_train.shape[1]\n",
    "num_batches = int(np.ceil(num_train_samples / batch_size))\n",
    "\n",
    "current_epoch = -1\n",
    "random_indexes = np.arange(num_train_samples)\n",
    "np.random.shuffle(random_indexes)\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"/Users/Antinomy/Desktop/tf_logs\"\n",
    "logdir = \"{}/run-{}\".format(root_logdir, now)\n",
    "\n",
    "# input_tensor = tf.placeholder(tf.float32, shape=(None, num_features), name=\"input_tensor\")\n",
    "# label = tf.placeholder(tf.float32, shape=(None, 1), name=\"label\")\n",
    "\n",
    "# define computation graph\n",
    "def logistic_regression(X):\n",
    "    W = tf.Variable(tf.random_uniform([2, 1], -1.0, 1.0), name=\"weights\")\n",
    "    b = tf.Variable(0.0, name=\"bias\")\n",
    "    output_tensor = tf.sigmoid(tf.add(tf.matmul(X, W), b))\n",
    "    return output_tensor\n",
    "\n",
    "input_tensor = tf.placeholder(tf.float32, shape=(None, 2), name=\"input_tensor\")\n",
    "label = tf.placeholder(tf.float32, shape=(None, 1), name=\"label\")\n",
    "y = logistic_regression(input_tensor)\n",
    "error = y - label\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    if os.path.exists(\"/Users/Antinomy/Desktop/tmp/checkpoint\"):\n",
    "        saver.restore(sess, \"/Users/Antinomy/Desktop/tmp/my_model.ckpt\")\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        for j in range(num_batches):\n",
    "            if i != current_epoch:\n",
    "                random_indexes = np.arange(num_train_samples)\n",
    "                np.random.shuffle(random_indexes)\n",
    "                current_epoch += 1\n",
    "                \n",
    "            selected_indexes = random_indexes[(j*batch_size):((j+1)*batch_size)]\n",
    "            X_train_batch, y_train_batch = X_train[selected_indexes], y_train[selected_indexes]\n",
    "            if j % 10 == 0:\n",
    "                summary_str = mse_summary.eval(feed_dict={input_tensor: X_train_batch, label: y_train_batch})\n",
    "                step = i*num_batches + j\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            \n",
    "            sess.run(training_op, feed_dict={input_tensor: X_train_batch, label: y_train_batch})\n",
    "        \n",
    "        print(\"---------- Epoch %d ----------\" % i)\n",
    "        print(\"Loss:\", mse.eval(feed_dict={input_tensor: X_train_batch, label: y_train_batch}))\n",
    "        save_path = saver.save(sess, \"/Users/Antinomy/Desktop/tmp/my_model.ckpt\")\n",
    "        \n",
    "    saver.save(sess, \"/Users/Antinomy/Desktop/final/my_final_model.ckpt\")\n",
    "    file_writer.close()\n",
    "    \n",
    "    # make predictions with the trained graph\n",
    "    predictions = y.eval(feed_dict={input_tensor: X_test})\n",
    "    final_predictions = [int(item > 0.5) for item in predictions]\n",
    "    print(\"========== Result on Test Set ==========\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, final_predictions))\n",
    "    print(\"AUC:\", roc_auc_score(y_test, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
