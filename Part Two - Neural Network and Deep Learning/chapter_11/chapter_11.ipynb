{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11: Training Deep Neural Network\n",
    "\n",
    "This chapter introduces some common techniques to train a deep neural network, including selecting initializers, choosing activation functions, batch normalization, using better optimizers, etc. This book only contains basic concept and usage of these techniques. For other techniques in training NN or the priciples of these methods, please refer to other materials.\n",
    "\n",
    "> This jupyter notebook contains my own solution to the coding exercises of the book. However the code may not be optimized to the best performance. For answers to the questions of the book, please check the markdown file under the same directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8: Deep Learning\n",
    "\n",
    "Requirement: \n",
    "1. Use 5 hidden layers with 100 neurons, He initializer and ELU activation function to build a DNN.\n",
    "2. Try to use Adam optimizer and early stopping to train the DNN on MNIST, but only with digit 0 ~ 4, because we need to train the model to recognize digit 5 ~ 9 using transfer learning in the next exercise. You will need an output softmax layer with 5 neurons, and make sure to keep saving the checkpoint for later usage.\n",
    "3. Use cross validation to fine tune hyperparameters. Check your precision score.\n",
    "4. Try batch normalization.\n",
    "5. Does the model overfit the train set? Try adding Dropout for each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below creates a five layer DNN and adds batch normalization and dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "\n",
    "# prepare for dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train_first, y_train_first, X_test_first, y_test_first = [], [], [], []\n",
    "X_train_second, y_train_second, X_test_second, y_test_second = [], [], [], []\n",
    "# select digits 0-4\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i] <= 4:\n",
    "        X_train_first.append(X_train[i])\n",
    "        y_train_first.append(y_train[i])\n",
    "    else:\n",
    "        X_train_second.append(X_train[i])\n",
    "        y_train_second.append(y_train[i])\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i] <= 4:\n",
    "        X_test_first.append(X_test[i])\n",
    "        y_test_first.append(y_test[i])\n",
    "    else:\n",
    "        X_test_second.append(X_test[i])\n",
    "        y_test_second.append(y_test[i])\n",
    "\n",
    "X_train_early = np.array(X_train_first)\n",
    "y_train_early = np.array(y_train_first)\n",
    "X_test_early = np.array(X_test_first)\n",
    "y_test_early = np.array(y_test_first)\n",
    "\n",
    "# hyperparameters\n",
    "num_epochs = 35\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_training_samples = X_train_early.shape[0]\n",
    "num_batches = int(np.ceil(num_training_samples / batch_size))\n",
    "random_indexes = np.arange(num_training_samples)\n",
    "np.random.shuffle(random_indexes)\n",
    "current_epoch = -1\n",
    "\n",
    "# build the network\n",
    "input_tensor = tf.placeholder(tf.float32, shape=(None, 28, 28), name=\"input\")\n",
    "reshape_input = tf.reshape(input_tensor, [-1, 28*28])\n",
    "label = tf.placeholder(tf.int32, shape=(None,), name=\"label\")\n",
    "\n",
    "def fully_connected_layer_with_elu_bn_dropout(\n",
    "    X,\n",
    "    num_neurons\n",
    "):\n",
    "    \"\"\"\n",
    "    A custom fully connected layer\n",
    "\n",
    "    :param X: input tensor\n",
    "    :param num_neurons: number of neuron in this custom fully connected layer\n",
    "    :return: output tensor with elu activation function, batch normalization and dropout to relief overfitting.\n",
    "\n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"my_fully_connected_layer\"):\n",
    "        hidden_layer = tf.layers.Dense(num_neurons, kernel_initializer=tf.initializers.he_normal())(X)\n",
    "        batch_mean, batch_var = tf.nn.moments(hidden_layer, axes=0)\n",
    "        hidden_layer = tf.nn.batch_normalization(hidden_layer, mean=batch_mean, variance=batch_var, offset=None, scale=None, variance_epsilon=0.0001)\n",
    "        hidden_layer = tf.nn.elu(hidden_layer)\n",
    "        hidden_layer = tf.nn.dropout(hidden_layer, rate=0.3)\n",
    "        return hidden_layer\n",
    "\n",
    "with tf.name_scope(\"fully_connected_network\"):\n",
    "    hidden_1 = fully_connected_layer_with_elu_bn_dropout(reshape_input, num_neurons=100)\n",
    "    hidden_2 = fully_connected_layer_with_elu_bn_dropout(hidden_1, num_neurons=100)\n",
    "    hidden_3 = fully_connected_layer_with_elu_bn_dropout(hidden_2, num_neurons=100)\n",
    "    hidden_4 = fully_connected_layer_with_elu_bn_dropout(hidden_3, num_neurons=100)\n",
    "    hidden_5 = fully_connected_layer_with_elu_bn_dropout(hidden_4, num_neurons=100)\n",
    "\n",
    "    logits = tf.layers.Dense(5, kernel_initializer=tf.initializers.he_normal(), name=\"output\")(hidden_5)\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=label, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name='loss')\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    adam_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = adam_optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        for j in range(num_batches):\n",
    "            if i != current_epoch:\n",
    "                random_indexes = np.arange(num_training_samples)\n",
    "                np.random.shuffle(random_indexes)\n",
    "                current_epoch += 1\n",
    "\n",
    "            selected_index = random_indexes[j*batch_size:(j+1)*batch_size]\n",
    "            X_train_batch, y_train_batch = X_train_early[selected_index], y_train_early[selected_index]\n",
    "            sess.run(train_op, feed_dict={input_tensor: X_train_batch, label: y_train_batch})\n",
    "\n",
    "        print(\"---------- Epoch %d ----------\" % (i))\n",
    "        print(\"Loss:\", loss.eval(feed_dict={input_tensor: X_train_batch, label: y_train_batch}))\n",
    "        save_path = saver.save(sess, \"/your/path/here\")\n",
    "\n",
    "    save_path = saver.save(sess, \"/your/path/here\")\n",
    "\n",
    "    # check the accuracy of trained model on train and test set\n",
    "    predictions = np.argmax(logits.eval(feed_dict={input_tensor: X_test_early, label: y_test_early}), axis=1)\n",
    "    print(\"========== Model Performance ==========\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test_early, predictions))\n",
    "    print(\"Precision::\", precision_score(y_test_early, predictions, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9: Transfer Learning\n",
    "\n",
    "Requirement:\n",
    "1. Reuse the hidden layers previously and build a new DNN.\n",
    "2. Train the new network with digit 5-9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 10: Pretrain on Assistant Task\n",
    "\n",
    "Requirement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
