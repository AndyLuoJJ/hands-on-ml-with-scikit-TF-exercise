{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7: Random Forest and Ensemble Learning\n",
    "\n",
    "This chapter introduces different kinds of ensemble methods, including voting, bagging, pasting, random forest, boosting and stacking. The book covers only how to use and fine-tune these methods. For further details on pinciples or advanced topics, please refer to other machine learning tutorials.\n",
    "\n",
    "> This notebook contains my solution to the programming exercises of chapter 7. For answers to other quesions, see the markdown file under the same folder. **Note that my code may not be fully tested or evaluated, for example, grid search and cross validation may not be performed. I only choose hyperparameters that gives an acceptable result.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8: Voting Classifier on MNIST\n",
    "\n",
    "Requirement: Create a voting classifier on MNIST dataset. Check the performance of individual classifiers and the ensemble one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's prepare the dataset. Split the MNIST dataset into train, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata(\"MNIST original\")\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "X_train, y_train = X[:50000], y[:50000]\n",
    "X_val, y_val = X[50000:60000], y[50000:60000]\n",
    "X_test, y_test = X[60000:], y[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create multiple base classifiers, including random forest, extreme randome tree and SVM. Train them with the same train set and evaluate them on validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit_transform(X_train)\n",
    "\n",
    "random_forest_clf = RandomForestClassifier(n_estimators=1000)\n",
    "random_forest_clf.fit(X_train, y_train)\n",
    "\n",
    "extra_tree_clf = ExtraTreesClassifier(n_estimators=1000)\n",
    "extra_tree_clf.fit(X_train, y_train)\n",
    "\n",
    "svm_clf = SVC(kernel=\"rbf\")\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "scaler.transform(X_val)\n",
    "random_forest_predictions_on_val = random_forest_clf.predict(X_val)\n",
    "print(\"Random Forest Precision:\", precision_score(y_val, random_forest_predictions_on_val))\n",
    "extra_tree_predictions_on_val = extra_tree_clf.predict(X_val)\n",
    "print(\"Extra Tree Precision:\", precision_score(y_val, extra_tree_predictions_on_val))\n",
    "svm_predictions_on_val = svm_clf.predict(X_val)\n",
    "print(\"SVM Precision:\", precision_score(y_val, svm_predictions_on_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the base estimators give class predictions rather than probabilities, we create a hard voting classifier to ensemble them. Check the performance of voting classifier on validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[(\"random_forest\", RandomForest(n_estimators=1000)), (\"extra_tree\", ExtraTreesClassifier(n_estimators=1000)), (\"svm\", SVC())],\n",
    "    voting=\"hard\"\n",
    ")\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "voting_predictions_on_val = voting_clf.predict(X_val)\n",
    "print(\"Voting Classifier Precision:\", precision_score(y_val, voting_predictions_on_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, check the performance of all models on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.transform(X_test)\n",
    "\n",
    "random_forest_predictions_on_test = random_forest_clf.predict(X_test)\n",
    "print(\"Random Forest Precision:\", precision_score(y_test, random_forest_predictions_on_test))\n",
    "\n",
    "extra_tree_predictions_on_test = extra_tree_clf.predict(X_test)\n",
    "print(\"Extra Trees Precision:\", precision_score(y_test, extra_tree_predictions_on_test))\n",
    "\n",
    "svm_predictions_on_test = svm_clf.predict(X_test)\n",
    "print(\"SVM Precision:\", precision_score(y_test, svm_predictions_on_test))\n",
    "\n",
    "voting_predictions_on_test = voting_clf.predict(X_test)\n",
    "print(\"Voting Precision:\", precision_score(y_test, voting_predictions_on_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9: Stacking on MNIST\n",
    "\n",
    "Requirement: Create a stacking model based on the basic classifiers above. Compare the performance of the stacking ensemble model with the voting classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we've already have the base estimators, we directly use them as basic estimators to create a stacking model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features = []\n",
    "for i in range(10000):\n",
    "    new_features.append([random_forest_predictions_on_val[i], extra_tree_predictions_on_val[i], svm_predictions_on_val[i]])\n",
    "    \n",
    "stacking_model = SVC(kernel=\"rbf\", C=1, gamma=\"scale\")\n",
    "stacking_model.fit(new_features, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the performance of the stacking ensemble model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_features = []\n",
    "for i in range(10000):\n",
    "    new_test_features.append([random_forest_predictions_on_test[i], extra_tree_predictions_on_test[i], svm_predictions_on_test[i]])\n",
    "    \n",
    "stacking_predicions_on_test = stacking_model.predict(new_test_features)\n",
    "print(\"Stacking Model Precision:\", precision_score(y_test, stacking_predicions_on_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
