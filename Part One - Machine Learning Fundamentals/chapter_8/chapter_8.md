# Chapter 8: Dimension Reduction

This chapter introduces some major dimension reduction algorithms, specificly PCA. For further details on advanced DR algorithms or deeper knowledge on PCA, please refer to other machine learning tutorials.

> This markdown file contains my answer to Ex1 - Ex8. For coding exercise, please check the jupyter notebook under the same directory.

1. 降低数据集维度的主要动机是什么？有什么主要弊端？

降低数据集维度的主要动机是加快训练速度、去除冗余信息。主要弊端是可能会带来一定程度的信息丢失、

> **Answer:** 
> 降维的主要动机是：
> - 为了加速后续的训练算法。在某些情况下，也可能为了消除噪声和冗余特征，使训练算法性能更好。
> - 为了将数据可视化，并从中获得洞察，了解最重要的特征。
> - 对数据进行压缩，节省空间。
> 降维的主要弊端是：
> - 丢失部分信息，可能使后续训练算法的性能降低。
> - 可能是计算密集型的。
> - 为机器学习流水线增添了些许复杂度。v
> - 转换后的特征往往难以解释。

2. 什么是维度的诅咒？

高维数据具有与低维数据截然不同的特性，在高维空间中的数据可能会非常稀疏，导致算法非常容易过拟合。

> **Answer:** 维度的诅咒是指许多在低维空间中不存在的问题，在高维空间中发生。一个常见的现象是随机抽样的高维向量通常非常稀疏，提升了过拟合的风险。

3. 一旦数据集被降维，是否还有可能逆转？

不能，降维必然会带来一定程度的信息损失，无法完全恢复降维前的数据集。

> **Answer:** 一旦对数据进行降维，几乎不可能再将操作完美地逆转，因为在降维过程中必然丢失了一部分信息。

4. PCA 可以用来给高度非线性数据集降维吗？

可以用核 PCA 对高度非线性数据集进行降维。

> **Answer:** 对大多数数据集来说，PCA 可以用来进行显著降维，即便是高度非线性的数据集，因为它至少可以消除无用的维度。但是如果不存在无用的维度，使用 PCA 将会损失太多信息。

5. 假设你在一个 1000 维数据集上执行 PCA，方差解释比维 95%。产生的结果数据集维度是多少？

产生的结果数据集维度取决于原数据集中数据的相关性，可能在1-1000之间。

> **Answer:** 取决于数据集，可能是 1 到 1000 之间的任何数字。

6. 常规 PCA、增量 PCA、随机 PCA 及核 PCA 各适用于何种情况？

- 常规 PCA：在内存足够的情况下，适用于大部分的数据降维任务。
- 增量 PCA：能够对小批量的数据集进行 PCA 处理，适用于在线学习或小批量训练的情况。
- 随机 PCA：相比于常规 PCA 的处理速度更快。
- 核 PCA：适用于对比较复杂的、非线性的数据集进行降维。

> **Answer:** 常规 PCA 是默认选择，但是仅适用于内存足够处理训练集的时候。增量 PCA 对于内存无法支持的大型数据集非常有用，但是速度比常规 PCA 慢一些，同时增量 PCA 对于在线任务同样有用。随机 PCA 适用于想要大大降低维度数量，并且内存能够支持数据集的情况。核 PCA 适用于处理非线性数据集。

7. 如何在你的数据集上评估降维算法的性能？

对降维前后的数据集进行分类/回归，并比较降维前后分类/回归模型的性能。一个较好的降维算法处理后的数据集，使用同一个模型的分类性能应该差距不大。

> **Answer:** 直观来说，如果降维算法能够消除许多维度并且不会丢失太多信息，那么这就是一个好的降维算法。如果将降维当作一个预处理过程，用在其他机器学习算法之前，那么可以通过简单测量第二个算法的性能来进行评估。如果降维过程没有损失太多信息，那么第二个算法的性能应该跟使用原始数据集一样好。

8. 链接两个不同的降维算法有意义吗？

有意义，两个不同的降维算法可能对同一批数据有不同的降维效果。

> **Answer:** 有意义。常见的例子是使用 PCA 快速去除大量无用的维度，然后应用另一种更慢的降维算法，如 LLE。这样两步走的策略产生的结果可能与仅使用 LLE 相同，但是时间要短得多。
