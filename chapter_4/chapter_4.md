# Chapter 4: Training Models

This chapter focuses on the theory of training machine learning models. Regression model is used as an example.

> No Python code is required in the exercises. Just answer the quesions and check the solutions.

1. 如果你的训练集有超过百万个特征，你会选择什么线性回归算法？

使用小批量梯度下降或随机梯度下降的线性回归算法。

> **Answer:** 可以使用随机梯度下降或小批量梯度下降算法；如果内存允许，甚至可以使用批量梯度下降算法。但是在特征非常多的情况下，不能使用标准方程的方法。

2. 如果你的训练集里特征的数值大小迥异，什么算法可能受到影响？受影响程度如何？你应该怎样做？

梯度下降算法会受到影响，某些特征的梯度可能比较小，导致算法收敛速度下降。
使用了正则化的算法也会受到影响，导致算法不稳定。
对于特征数值大小迥异的情况，可以对特征进行标准化，将特征的值映射为较小范围内的值。

> **Answer:** 会导致梯度下降算法耗费较长时间收敛。需要在训练前对数据进行缩放。

3. 训练逻辑回归模型时，梯度下降是否会困于局部最小值？

不会，逻辑回归模型的损失函数是一个凸函数，一定能够收敛到全局最小。

> **Answer:** 不会，因为成本函数为凸函数。

4. 假设运行时间足够长，所有的梯度下降算法是不是最终都会产生相同的模型？

不一定。如果损失函数存在多个局部极小值，使用不同初始化策略可能会导致模型发生改变；但如果损失函数为凸函数，则梯度下降算法最终会得到相同的模型（随机梯度下降可能会在极小值附近波动）。

> **Answer:** 如果优化问题是凸的，并且学习率不是太高，那么所有梯度下降算法都会接近全局最优。但除非逐渐减小学习率，否则随机梯度下降和小批量梯度下降不会真正收敛，只会在全局最优附近波动。

5. 假设你使用的是批量梯度下降，并且每一轮训练都绘制出其验证误差，如果发现验证误差持续上升，可能发生了什么？你如何解决这个问题？

模型可能出现过拟合。可以采用正则化、早期停止等方法

> **Answer:** 如果训练误差也逐渐上升，一个可能的原因是学习率太大。但如果训练误差没有上升，则可能出现过拟合。

6. 当验证误差开始上升时，立刻停止小批量梯度下降算法训练是否为一个好的主意？

不一定，可能在下两个batch的位置验证误差又重新下降了。可以观察验证误差在几个batch后是否仍然上升，即推迟早期停止的时刻。

> **Answer:** 如果在验证误差刚开始上升时就停止训练，很有可能会在达到最优之前过早停止训练。更好的办法是定时保存模型，当较长一段时间都没有改善时，可以恢复到保存的最优模型。

7. 哪种梯度下降算法能最快到达最优解附近？哪种会收敛？如何使其他算法同样收敛？

随机梯度下降能最快到达最优解附近。
批量梯度下降一定能够收敛。
对于随机梯度下降和小批量梯度下降算法，需要逐渐减小学习率才能够收敛。

> **Answer:** 随机梯度下降一次只考虑一个实例，因此训练迭代最快；但只有批量梯度下降才会经过足够长时间的训练后真正收敛。对于随机梯度下降和小批量梯度下降来说，除非逐渐调低学习率，否则将一直围绕最小值上上下下。

8. 假设你使用的是多项式回归，绘制出学习曲线，你发现训练误差和验证误差之间存在很大的差距。发生了什么？哪三种方法可以解决这个问题？

过拟合。解决方法：增加训练数据；引入正则化；降低模型复杂度。

> **Answer:** 如果验证误差远高于训练误差，可能是因为模型过度拟合训练集。解决方法之一是对多项式降阶：自由度越低的模型，过度拟合的可能性越低。另一个方法是对模型进行正则化——采用L1或L2惩罚。最后，还可以尝试扩大训练集。

9. 假设你使用的是岭回归，你注意到训练误差和验证误差几乎相等，并且非常高，你认为模型是高方差还是高偏差？你应该提高还是降低正则化超参数？

高偏差。应该降低正则化超参数，减轻欠拟合。

> **Answer:** 模型很可能对训练集拟合不足，这意味着偏差较高，应该尝试降低正则化超参数。

10. 你为何要使用
    - 岭回归而不是线性回归？
    - Lasso 回归而不是岭回归？
    - 弹性网络而不是 Lasso 回归？

岭回归相比线性回归，能够使模型参数限制在 0 附近，可以减轻模型过拟合的程度。
Lasso 回归相比岭回归，能够进行一定程度的特征筛选，将不重要的特征的权重变为 0。
弹性网络相比 Lasso 回归，在特征数量比较多的时候更加稳定，并且弹性网络综合了岭回归和 Lasso 回归，可以通过调节系数实现两者的转换，

> **Answer:** 
> 1. 有正则化的模型通常比没有正则化的模型表现得更好，所以应该优先选择岭回归而不是普通的线性回归。
> 2. Lasso回归采用L1惩罚函数，往往倾向于将不重要的特征权重降为0，这将生成一个除了最重要的权重之外，其他权重都为零的稀疏模型。这是自动执行特征选择的一种方法。
> 3. 某些情况下Lasso可能产生异常表现（例如当多个特征强相关，或者特征数量比训练实例多时），而弹性网络会添加一个额外的超参数对模型进行调整。可以通过设置超参数使弹性网络等效于Lasso回归或岭回归

11. 如果你想将图片分类为户外/室内以及白天/黑夜。你应该实现两个逻辑回归分类器还是一个softmax回归分类器？

应该实现两个逻辑回归分类器，softmax回归分类器是一个多类别分类器，但不是一个多输出分类器。

> **Answer:** 应该训练两个逻辑回归分类器，因为这些类别之间并不是排他的（存在四种组合）。
